# ðŸ”® Funcionalidade Futura: Scrapy Integration

> **Status:** Planejado

## Conceito

Integrar Scrapy ao projeto para habilitar **web scraping em escala** e **pipelines de dados** (ETL/ELT).

## Por que Scrapy?

| Problema | SoluÃ§Ã£o Scrapy |
|----------|----------------|
| Playwright Ã© lento para muitas pÃ¡ginas | Scrapy usa requests HTTP diretas (10x+ mais rÃ¡pido) |
| Dados precisam ir para banco/API | Pipelines nativos para transformaÃ§Ã£o e exportaÃ§Ã£o |
| Sites bloqueiam por rate-limit | Middlewares de delay, rotaÃ§Ã£o de User-Agent, proxies |

## Arquitetura Planejada

```
project/
â”œâ”€â”€ scrapy/
â”‚   â”œâ”€â”€ spiders/          # Spiders por domÃ­nio/funcionalidade
â”‚   â”‚   â””â”€â”€ exemplo_spider.py
â”‚   â”œâ”€â”€ pipelines/        # TransformaÃ§Ã£o e destino dos dados
â”‚   â”‚   â”œâ”€â”€ clean_pipeline.py      # Limpeza/normalizaÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ database_pipeline.py   # Salva em banco
â”‚   â”‚   â””â”€â”€ api_pipeline.py        # Envia para API
â”‚   â”œâ”€â”€ middlewares/      # Interceptadores de request/response
â”‚   â””â”€â”€ settings.py       # ConfiguraÃ§Ã£o global
```

## Pipeline ETL vs ELT

### ETL (Extract â†’ Transform â†’ Load)
```
Scrapy Spider â†’ Clean Pipeline â†’ Database Pipeline
```
- Transforma os dados **antes** de salvar
- Bom para bancos relacionais com schema rÃ­gido

### ELT (Extract â†’ Load â†’ Transform)
```
Scrapy Spider â†’ Raw Storage â†’ DBT/SQL Transforms
```
- Salva dados brutos, transforma depois
- Bom para Data Lakes e anÃ¡lises flexÃ­veis

## IntegraÃ§Ã£o com Gen Food

O `gen_food` identifica os elementos. O Scrapy usa esses seletores para extrair dados em escala:

```python
# Spider usando seletores do food.json
class ExemploSpider(scrapy.Spider):
    def parse(self, response):
        # Seletores vieram do food.json
        yield {
            "titulo": response.css("#titulo::text").get(),
            "preco": response.css("[data-testid='preco']::text").get(),
        }
```

## PrÃ³ximos Passos

1. Adicionar `scrapy` ao `requirements.txt`
2. Criar estrutura de pastas `project/scrapy/`
3. Implementar spider de exemplo
4. Documentar pipelines disponÃ­veis

---

*Esta funcionalidade ainda nÃ£o estÃ¡ implementada.*
