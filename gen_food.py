#!/usr/bin/env python3
"""
Gen Food - Coletor de dados para automa√ß√£o.

Coleta snapshot da p√°gina, invent√°rio de elementos interativos
e candidatos de seletores para melhorar PageObjects.

Modo interact: grava a√ß√µes do usu√°rio em tempo real.

N√ÉO gera scripts de replay ou PageObjects autom√°ticos.
"""

import argparse
import json
import signal
import sys
from datetime import datetime, timezone
from pathlib import Path

from project.core import (
    get_config,
    create_run_dirs,
    generate_run_id,
    get_logger,
    create_browser_context,
    close_browser,
    extract_elements,
    capture_snapshot,
    ActionRecorder,
    setup_recorder,
)
from project.core.log import setup_file_logging, close_file_logging

logger = get_logger("gen_food")

SCHEMA_VERSION = "1.0"


def run_snapshot(
    url: str,
    dirs: dict,
    run_id: str,
    headless: bool,
    profile_dir: str | None,
    mask_sensitive: bool,
) -> dict:
    """
    Executa coleta em modo snapshot.
    
    Args:
        url: URL para coletar.
        dirs: Dicion√°rio de diret√≥rios do run.
        run_id: ID da execu√ß√£o.
        headless: Se True, roda sem janela.
        profile_dir: Diret√≥rio de perfil para sess√£o persistente.
        mask_sensitive: Se True, mascara dados sens√≠veis.
    
    Returns:
        Dicion√°rio com resultados da coleta.
    """
    logger.info(f"Iniciando snapshot de: {url}")
    
    # Cria browser
    browser, context, page = create_browser_context(
        headless=headless,
        profile_dir=profile_dir,
    )
    
    try:
        # Navega para URL
        page.goto(url, wait_until="networkidle", timeout=30000)
        logger.info(f"P√°gina carregada: {page.title()}")
        
        # Aguarda um pouco para JS carregar
        page.wait_for_timeout(2000)
        
        # Captura snapshot
        html_path = dirs["html"] / "page.html"
        screenshot_path = dirs["screenshots"] / "page.png"
        capture_snapshot(page, str(html_path), str(screenshot_path))
        
        # Extrai elementos
        extraction = extract_elements(page, mask_sensitive=mask_sensitive)
        
        # Monta food.json
        timestamp = datetime.now(timezone.utc).isoformat()
        food_data = {
            "schema_version": SCHEMA_VERSION,
            "url": url,
            "run_id": run_id,
            "timestamp": timestamp,
            "page_signals": extraction["page_signals"],
            "elements": extraction["elements"],
        }
        
        # Salva food.json
        food_path = dirs["food"] / "food.json"
        with open(food_path, "w", encoding="utf-8") as f:
            json.dump(food_data, f, indent=2, ensure_ascii=False)
        logger.info(f"Food salvo: {food_path}")
        
        return {
            "success": True,
            "elements_count": len(extraction["elements"]),
            "page_signals": extraction["page_signals"],
        }
        
    except Exception as e:
        logger.error(f"Erro no snapshot: {e}")
        return {
            "success": False,
            "error": str(e),
        }
        
    finally:
        close_browser(browser, context)


def run_interact(
    url: str,
    dirs: dict,
    run_id: str,
    headless: bool,
    profile_dir: str | None,
    mask_sensitive: bool,
) -> dict:
    """
    Modo interact - grava a√ß√µes do usu√°rio.
    
    Abre browser para intera√ß√£o manual, grava todas as a√ß√µes,
    e finaliza ao fechar o browser ou pressionar Ctrl+C.
    
    Args:
        url: URL para navegar.
        dirs: Dicion√°rio de diret√≥rios do run.
        run_id: ID da execu√ß√£o.
        headless: Ignorado - interact sempre usa headed.
        profile_dir: Diret√≥rio de perfil para sess√£o persistente.
        mask_sensitive: Se True, mascara dados sens√≠veis.
    
    Returns:
        Dicion√°rio com resultados da coleta.
    """
    # Interact sempre usa headed (precisa de janela para intera√ß√£o)
    if headless:
        logger.warning("Modo interact ignora --headless (precisa de janela)")
    
    logger.info(f"Iniciando modo INTERACT para: {url}")
    logger.info("=" * 60)
    logger.info("üé¨ GRAVA√á√ÉO ATIVA")
    logger.info("Interaja com a p√°gina normalmente.")
    logger.info("Para finalizar: feche o browser ou pressione Ctrl+C")
    logger.info("=" * 60)
    
    # Cria gravador de a√ß√µes
    actions_path = dirs["food"] / "actions.ndjson"
    recorder = ActionRecorder(actions_path, mask_sensitive=mask_sensitive)
    recorder.start()
    
    # Cria browser (sempre headed)
    browser, context, page = create_browser_context(
        headless=False,  # Sempre headed
        profile_dir=profile_dir,
    )
    
    # Flag para controle de interrup√ß√£o
    interrupted = False
    
    def handle_interrupt(sig, frame):
        nonlocal interrupted
        interrupted = True
        logger.info("\nüõë Ctrl+C detectado. Finalizando...")
    
    # Registra handler de Ctrl+C
    original_handler = signal.signal(signal.SIGINT, handle_interrupt)
    
    # Sistema de pageId - mapeia URL normalizada para pageId
    pages_visited: dict = {}  # {url_normalizada: {"pageId": int, "title": str, "first_visit": str}}
    next_page_id = 1
    current_url = ""
    
    def normalize_url(raw_url: str) -> str:
        """Normaliza URL removendo query strings e fragments para compara√ß√£o."""
        from urllib.parse import urlparse, urlunparse
        parsed = urlparse(raw_url)
        # Mant√©m scheme, netloc, path - remove query e fragment
        return urlunparse((parsed.scheme, parsed.netloc, parsed.path, "", "", ""))
    
    def get_or_create_page_id(raw_url: str, title: str = "") -> tuple:
        """
        Retorna (pageId, is_new) para a URL.
        Se URL j√° foi visitada, retorna pageId existente e is_new=False.
        """
        nonlocal next_page_id
        normalized = normalize_url(raw_url)
        
        if normalized in pages_visited:
            return pages_visited[normalized]["pageId"], False
        
        # Nova p√°gina
        page_id = next_page_id
        next_page_id += 1
        pages_visited[normalized] = {
            "pageId": page_id,
            "url": raw_url,
            "title": title,
            "first_visit": datetime.now(timezone.utc).isoformat(),
        }
        return page_id, True
    
    def capture_page_snapshot(page_obj, page_id: int, is_new: bool) -> None:
        """
        Captura screenshot (sempre) e HTML (s√≥ se p√°gina nova).
        Screenshot: <timestamp>_page_<pageId>.png
        HTML: page_<pageId>.html (s√≥ se is_new)
        """
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
        
        # Screenshot sempre com timestamp
        screenshot_name = f"{timestamp}_page_{page_id}.png"
        screenshot_path = dirs["screenshots"] / screenshot_name
        try:
            page_obj.screenshot(path=str(screenshot_path), full_page=True)
            logger.info(f"Screenshot salvo: {screenshot_path.name}")
        except Exception as e:
            logger.warning(f"Erro ao salvar screenshot: {e}")
        
        # HTML s√≥ se p√°gina nova (n√£o repetida)
        if is_new:
            html_name = f"page_{page_id}.html"
            html_path = dirs["html"] / html_name
            try:
                html_content = page_obj.content()
                with open(html_path, "w", encoding="utf-8") as f:
                    f.write(html_content)
                logger.info(f"HTML salvo: {html_path.name}")
            except Exception as e:
                logger.warning(f"Erro ao salvar HTML: {e}")
    
    try:
        # Configura gravador no browser
        setup_recorder(context, page, recorder)
        
        # Navega para URL (usa 'load' ao inv√©s de 'networkidle' para SPAs)
        page.goto(url, wait_until="load", timeout=30000)
        title = page.title()
        logger.info(f"P√°gina carregada: {title}")
        
        # Registra p√°gina inicial
        current_url = page.url
        page_id, is_new = get_or_create_page_id(current_url, title)
        logger.info(f"P√°gina ID: {page_id} (nova: {is_new})")
        
        # Captura inicial
        capture_page_snapshot(page, page_id, is_new)
        
        # Loop aguardando fechamento ou interrup√ß√£o
        logger.info("Aguardando intera√ß√µes...")
        
        while not interrupted:
            try:
                # Verifica se ainda h√° p√°ginas abertas no contexto
                if not context.pages:
                    logger.info("Browser fechado pelo usu√°rio.")
                    break
                
                # Pega a p√°gina ativa atual (pode mudar durante navega√ß√£o SPA)
                current_page = context.pages[-1]
                
                # Verifica se a p√°gina responde
                current_page.evaluate("1")
                
                # Verifica se a URL mudou (navega√ß√£o)
                new_url = current_page.url
                if new_url != current_url:
                    current_url = new_url
                    page = current_page
                    title = page.title()
                    
                    page_id, is_new = get_or_create_page_id(current_url, title)
                    logger.info(f"Navega√ß√£o detectada: {current_url[:50]}... ‚Üí Page ID: {page_id}")
                    
                    # Captura screenshot (sempre) e HTML (s√≥ se nova)
                    capture_page_snapshot(page, page_id, is_new)
                
                current_page.wait_for_timeout(500)
                    
            except Exception as e:
                # Verifica se √© realmente fechamento ou s√≥ erro tempor√°rio
                try:
                    if not context.pages:
                        logger.info("Browser fechado pelo usu√°rio.")
                        break
                    # Se ainda h√° p√°ginas, pode ser navega√ß√£o - tenta continuar
                    page = context.pages[-1]
                    continue
                except Exception:
                    logger.info("Browser fechado pelo usu√°rio.")
                    break
        
        # Tenta captura final
        try:
            if context.pages:
                page = context.pages[-1]
                page_id, is_new = get_or_create_page_id(page.url, page.title())
                capture_page_snapshot(page, page_id, is_new)
        except Exception:
            logger.info("Usando √∫ltima captura (browser j√° fechado).")
        
        # Extrai elementos da √∫ltima p√°gina
        extraction = {"page_signals": {}, "elements": []}
        try:
            if context.pages:
                extraction = extract_elements(page, mask_sensitive=mask_sensitive)
        except Exception as e:
            logger.warning(f"N√£o foi poss√≠vel extrair elementos: {e}")
        
        # Para grava√ß√£o
        actions = recorder.stop()
        summary = recorder.get_summary()
        
        # Monta lista de p√°ginas visitadas com pageId
        urls_visited_with_id = [
            {
                "pageId": info["pageId"],
                "url": info["url"],
                "title": info["title"],
                "first_visit": info["first_visit"],
            }
            for info in pages_visited.values()
        ]
        
        # Monta food.json
        timestamp = datetime.now(timezone.utc).isoformat()
        food_data = {
            "schema_version": SCHEMA_VERSION,
            "url": url,
            "run_id": run_id,
            "timestamp": timestamp,
            "mode": "interact",
            "pages_visited": urls_visited_with_id,
            "page_signals": extraction["page_signals"],
            "elements": extraction["elements"],
            "action_summary": {
                **summary,
                "total_pages": len(pages_visited),
            },
        }
        
        # Salva food.json
        food_path = dirs["food"] / "food.json"
        with open(food_path, "w", encoding="utf-8") as f:
            json.dump(food_data, f, indent=2, ensure_ascii=False)
        logger.info(f"Food salvo: {food_path}")
        
        return {
            "success": True,
            "elements_count": len(extraction["elements"]),
            "actions_count": summary["total_actions"],
            "pages_count": len(pages_visited),
            "action_types": summary["action_types"],
            "page_signals": extraction["page_signals"],
        }
        
    except Exception as e:
        logger.error(f"Erro no interact: {e}")
        
        # Tenta salvar o que foi capturado mesmo com erro
        try:
            actions = recorder.stop()
            summary = recorder.get_summary()
            
            # Se temos a√ß√µes, considera parcialmente sucesso
            if summary["total_actions"] > 0:
                # Salva food.json com o que temos
                food_data = {
                    "schema_version": SCHEMA_VERSION,
                    "url": url,
                    "run_id": run_id,
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "mode": "interact",
                    "page_signals": {},
                    "elements": [],
                    "action_summary": summary,
                    "error": str(e),
                }
                food_path = dirs["food"] / "food.json"
                with open(food_path, "w", encoding="utf-8") as f:
                    json.dump(food_data, f, indent=2, ensure_ascii=False)
                logger.info(f"Food salvo (parcial): {food_path}")
                
                return {
                    "success": True,
                    "partial": True,
                    "elements_count": 0,
                    "actions_count": summary["total_actions"],
                    "action_types": summary["action_types"],
                    "page_signals": {},
                    "warning": str(e),
                }
        except Exception:
            pass
        
        return {
            "success": False,
            "error": str(e),
        }
        
    finally:
        # Restaura handler original
        signal.signal(signal.SIGINT, original_handler)
        close_browser(browser, context)


def main():
    parser = argparse.ArgumentParser(
        description="Gen Food - Coletor de dados para automa√ß√£o",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos:
  # Snapshot r√°pido
  python gen_food.py --url https://example.com
  
  # Modo interativo (grava suas a√ß√µes)
  python gen_food.py --url https://example.com --mode interact
  
  # Com sess√£o persistente (mant√©m login)
  python gen_food.py --url https://example.com --profile-dir ./profile
        """,
    )
    
    parser.add_argument(
        "--url",
        type=str,
        required=False,
        help="URL alvo para coletar dados",
    )
    
    parser.add_argument(
        "--mode",
        type=str,
        default="snapshot",
        choices=["snapshot", "interact"],
        help="Modo de coleta: snapshot (r√°pido) ou interact (grava a√ß√µes)",
    )
    
    parser.add_argument(
        "--headless",
        action="store_true",
        help="Rodar browser sem janela vis√≠vel (apenas snapshot)",
    )
    
    parser.add_argument(
        "--profile-dir",
        type=str,
        default=None,
        help="Diret√≥rio de perfil para manter sess√£o/login entre execu√ß√µes",
    )
    
    parser.add_argument(
        "--no-mask",
        action="store_true",
        help="‚ö†Ô∏è  ATEN√á√ÉO: Desabilita mascaramento de dados sens√≠veis (passwords)",
    )
    
    args = parser.parse_args()
    
    # Carrega configura√ß√£o
    config = get_config()
    url = args.url or config.base_url
    
    if not url:
        logger.error("Nenhuma URL fornecida. Use --url ou defina BASE_URL no .env")
        return 1
    
    # Warning se mascaramento desabilitado
    mask_sensitive = not args.no_mask
    if not mask_sensitive:
        logger.warning("=" * 60)
        logger.warning("‚ö†Ô∏è  MASCARAMENTO DESABILITADO!")
        logger.warning("Dados sens√≠veis (passwords) ser√£o salvos em texto claro!")
        logger.warning("Use apenas para debugging em ambiente seguro.")
        logger.warning("=" * 60)
    
    # Gera run ID e cria diret√≥rios
    run_id = generate_run_id()
    dirs = create_run_dirs(config.artifacts_dir, run_id)
    
    # Configura logging para arquivo
    log_path = dirs["logs"] / "session.log"
    setup_file_logging(log_path)
    
    started_at = datetime.now(timezone.utc).isoformat()
    
    logger.info(f"Run ID: {run_id}")
    logger.info(f"URL: {url}")
    logger.info(f"Modo: {args.mode}")
    logger.info(f"Headless: {args.headless}")
    logger.info(f"Profile dir: {args.profile_dir or '(nenhum)'}")
    logger.info(f"Run dir: {dirs['run']}")
    logger.info(f"Log: {log_path}")
    
    # Executa modo selecionado
    if args.mode == "snapshot":
        result = run_snapshot(
            url=url,
            dirs=dirs,
            run_id=run_id,
            headless=args.headless,
            profile_dir=args.profile_dir,
            mask_sensitive=mask_sensitive,
        )
    else:
        result = run_interact(
            url=url,
            dirs=dirs,
            run_id=run_id,
            headless=args.headless,
            profile_dir=args.profile_dir,
            mask_sensitive=mask_sensitive,
        )
    
    # Salva meta.json
    meta = {
        "schema_version": SCHEMA_VERSION,
        "run_id": run_id,
        "started_at": started_at,
        "url": url,
        "mode": args.mode,
        "headless": args.headless if args.mode == "snapshot" else False,
        "profile_dir_used": args.profile_dir is not None,
        "mask_sensitive": mask_sensitive,
        "result": result,
    }
    
    meta_path = dirs["run"] / "meta.json"
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta, f, indent=2, ensure_ascii=False)
    
    logger.info(f"Meta salvo: {meta_path}")
    
    if result.get("success"):
        elements = result.get('elements_count', 0)
        actions = result.get('actions_count', 0)
        pages = result.get('pages_count', 1)
        if args.mode == "interact":
            logger.info(f"‚úÖ Coleta conclu√≠da! {elements} elementos, {actions} a√ß√µes, {pages} p√°ginas.")
        else:
            logger.info(f"‚úÖ Coleta conclu√≠da! {elements} elementos extra√≠dos.")
        close_file_logging()
        return 0
    else:
        logger.error(f"‚ùå Coleta falhou: {result.get('error', 'erro desconhecido')}")
        close_file_logging()
        return 1


if __name__ == "__main__":
    sys.exit(main())
